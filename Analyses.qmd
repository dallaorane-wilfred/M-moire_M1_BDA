---
title: "Analyses"
author: "DALLAORANE Wilfred"
format: 
  html:
    theme: flatly      # Choisir un th√®me Quarto 
    toc: true          # Activer la table des mati√®res
    toc-depth: 3       # Profondeur de la table des mati√®res
    self-contained: true # Int√©grer les graphiques dans le HTML
    toc-title: "Sommaire"  
    code-fold: true      # Permet de replier/d√©plier le code
    code-tools: true      # Ajoute des outils pour copier le code
execute:
  echo: true
  warning: false
  message: false
  results: "markup" 
---

```{r}
library(knitr)
options(knitr.table.format = "html")  # Assure un bon affichage en HTML

```

# 1. Pr√©sentation des donn√©es r√©elles et analyses statistiques

Dans cette √©tude, nous utilisons le jeu de donn√©es KG introduit par Klein et Goldberger (1964) dans leur ouvrage *An Economic Model of the United States, 1929-1952*. Ce jeu de donn√©es est particuli√®rement reconnu dans la litt√©rature √©conom√©trique pour pr√©senter des probl√®mes de multicolin√©arit√©. Notre objectif √©tant d'√©tudier l‚Äôefficacit√© de la r√©gression Ridge par rapport aux Moindres Carr√©s Ordinaires (MCO), ce jeu de donn√©es en fait une r√©f√©rence appropri√©e pour notre √©tude. Nous allons identifier les d√©terminants de la consommation.\

## Importation des donn√©es KG

Nous importons les donn√©es et examinons leurs premi√®res lignes.

```{r}
# Charger les packages n√©cessaires
library(multiColl)
library(tidyverse)
library(modelsummary)
library("kableExtra")
library(glmnet)
library(gt)
library(boot)
library(lmridge)
library(ggplot2)
library(reshape2)
library(ggcorrplot)
library(car)
```

```{r}
# Importer les donn√©es KG
data("KG")
# Aper√ßu des donn√©es
head(KG)
```

Il est constitu√© de 14 obsevations et l'ensemble des variables comprend:

-   **Consommation (consumption)** : Cette variable repr√©sente la consommation domestique des m√©nages, un indicateur cl√© de la demande agr√©g√©e dans l‚Äô√©conomie am√©ricaine.


-   **Revenu salarial (wage.income)** : Revenu qui provient des activit√©s salari√©es.

-   **Revenu non agricole (non.farm.income)** : Revenu provenant √† partir d‚Äôactivit√©s autres que l'agriculture et emplois salari√©s. Cela pourrait √™tre des revenus issus d‚Äôentreprises ind√©pendantes ou d‚Äôinvestissements.

-   **Revenu agricole (farm.income)** : Revenus provenant des activit√©s agricoles, principalement, repr√©sentant une variable importante dans l'√©conomie des √âtats-Unis pendant la p√©riode √©tudi√©e.\*

Ces variables devraient avoir un effet positif sur la consommation. Nous admettons que ces observations correspondent √† des donn√©es agr√©g√©es, exprim√©es en millions de dollars.

Pr√©sentons les statistiques descriptives des variables √©tudi√©es, afin de d√©crire leurs natures.

```{r}
# Calculer les statistiques descriptives
summary_stats <- data.frame(
  Moyenne = sapply(KG, mean),
  Mediane = sapply(KG, median),
  EcartType = sapply(KG, sd),
  Minimum = sapply(KG, min),
  Maximum = sapply(KG, max)
)

# Ajouter les noms des variables comme une colonne, en √©liminant les conflits √©ventuels
summary_stats <- tibble::rownames_to_column(summary_stats, var = "Variable")



# Cr√©ation du tableau avec gt
summary_table <- summary_stats %>%
  gt() %>%
  tab_header(
    title = md("**Statistiques Descriptives des Variables KG**"),
    subtitle = "R√©sum√© des principales mesures de tendance centrale et de dispersion"
  ) %>%
  fmt_number(
    columns = c(Moyenne, Mediane, EcartType, Minimum, Maximum),
    decimals = 3
  ) %>%
  cols_label(
    Variable = "Variables",
    Moyenne = "Moyenne",
    Mediane = "M√©diane",
    EcartType = "√âcart-Type",
    Minimum = "Min",
    Maximum = "Max"
  ) %>%
  tab_options(
    table.width = pct(70),
    column_labels.font.weight = "bold",
    heading.align = "center"
  ) 

# Affichage du tableau
summary_table


```

Les d√©penses en consommation varient de 62,80 et 111,40 millions de dollars avec une moyenne de 87,12 millions de dollars, une m√©diane de 91 millions de dollars et un √©cart-type de 18,64 millions de dollars. Les r√©sultats montrent √©galement que le revenu salarial moyen est de 68,38 millions de de dollars r√©presentant ainsi la principale source de revenu de la population √©tudi√©es. Il varie entre 43,41 et 95,47 millions de dollars avec un √©cart-type de 18,88 millions de dollars et une mediane de 76,32 millions de dollars. Quant au revenu non agricole, il varie entre 17,09 et 37,58 millions de dollars avec une moyenne de 27,28 millions de dollars, une mediane de 28,18 millions de dollars et un √©cart-type de 7,09 millions de dollars. Enfin, le revenu agricole moyen est de 6,96 millions de dollars, une mediane de 7,3 millions de dollars (proche de la moyenne, indiquant une distribution sym√©trique), un √©cart-type de 2,07 millions de dollars. Il varie entre 3,96 et 9,85 millions de dollars.

Les √©cart-types montrent que les variables **wage.income** et **consumption** pr√©sentent une variabilit√© importante, ce qui pourrait influencer les r√©sultats des r√©gressions. Tandis que les variables **non.farm.income** et **farm.income** pr√©sentent une dispersion faible.

Ces observations confirment l'importance d'une analyse pour d√©tecter les √©ventuelles relations multicolin√©aires entre ces variables, √©tant donn√© leur interd√©pendance √©conomique potentielle.

-   **Corr√©lations**

```{r}
# Calcul de la matrice de corr√©lation
cor_matrix <- cor(KG, use = "pairwise.complete.obs")

# Convertir en format long et filtrer uniquement le triangle inf√©rieur
cor_data <- as.data.frame(as.table(cor_matrix)) %>%
  rename(Variable1 = Var1, Variable2 = Var2, Correlation = Freq) %>%
  filter(as.numeric(Variable1) >= as.numeric(Variable2))  # Garder uniquement le triangle inf√©rieur

# Transformer en format large pour affichage triangulaire
cor_wide <- cor_data %>%
  pivot_wider(names_from = Variable2, values_from = Correlation)

# Ajouter une colonne pour les noms des variables
cor_wide <- cor_wide %>%
  rename(Variables = Variable1)

# Remplacer les NA par des vides uniquement dans l'affichage `gt()` avec `fmt_missing()`
cor_table <- cor_wide %>%
  gt() %>%
  tab_header(
    title = md("**Matrice de Corr√©lation des Variables KG**")
  ) %>%
  fmt_number(
    columns = where(is.numeric),  # Appliquer le formatage aux colonnes num√©riques
    decimals = 3
  ) %>%
  fmt_missing(
    columns = everything(),  # Remplace les valeurs NA uniquement dans l'affichage
    missing_text = ""
  ) %>%
  cols_label(
    Variables = "Variables"
  ) %>%
  tab_style(
    style = cell_fill(color = "lightblue"),
    locations = cells_column_labels()
  ) %>%
  tab_options(
    table.width = pct(80),
    column_labels.font.weight = "bold",
    heading.align = "center"
  )

# Affichage du tableau
cor_table
# ---- Heatmap triangulaire ----

# Masquer la partie sup√©rieure de la matrice
cor_matrix[upper.tri(cor_matrix)] <- NA

# Heatmap triangulaire
ggcorrplot(cor_matrix, type = "lower", lab = TRUE, colors = c("blue", "white", "red")) +
  ggtitle("Corr√©lations entre les Variables ")
```

Les corr√©lations affich√©es dans la matrice montrent des niveaux √©lev√©s de corr√©lation entre plusieurs variables explicatives, ce qui sugg√®re une forte multicolin√©arit√© potentielle. Wage.income et non.farm.income pr√©sentent une corr√©lation de 0.943, tandis que wage.income et consumption sont corr√©l√©es √† hauteur de 0.942. De plus, non.farm.income et consumption affichent une corr√©lation de 0.946. Farm.income pr√©sente une corr√©lation mod√©r√©e avec les autres variables, variant entre 0.737 et 0.811. Ces valeurs proches de 1 indiquent une forte interd√©pendance entre ces variables, ce qui est un indicateur clair de multicolin√©arit√© s√©v√®re.

# 2. Estimations

Dans cette section, nous allons faire une estimation par les moindres carr√©s, et appliquer la regression Ridge.

## a. Estimation par les Moindres Carr√©s Ordinaires.

### √âquation √† estimer :

$$
\text{consumption}_i = \beta_0 + \beta_1 \cdot \text{wage.income}_i + \beta_2 \cdot \text{non.farm.income}_i + \beta_3 \cdot \text{farm.income}_i + \epsilon_i
$$

```{r}
# Cr√©ez le mod√®le de r√©gression
model_mco <- lm(consumption ~ wage.income + non.farm.income + farm.income, data = KG)

# R√©sum√© classique pour tester
summary(model_mco)

# R√©sum√© avec modelsummary
modelsummary(model_mco, stars = c('*'= .1, '**'=.05, '***'=0.01))
```

## Interpr√©tation

-   Qualit√© globale du mod√®le :

    -   **R¬≤ = 0.9187** : Le mod√®le explique 91,87 % de la variance de la consommation.

    -   **F-statistic (p \< 0.001)** : Le mod√®le est globalement significatif, au moins une des variables explicatives a un impact significatif sur la consommation.

-   **Analyse des coefficients :**

    -   La consommation moyenne est de 18,7 millions de dollars lorsque toutes les variables explicatives sont nulles.

    -   **wage.income** : Coefficient = 0.3803, non significatif (p = 0,2511)

    -   **non.farm.income** : Coefficient = 1.4186, significatif (p = 0.0772). Une augmentation du revenu non agricol de 1 millions de dollars augmente la consommation de 1,42 millions de dollars, citeris paribus.

    -   **farm.income** : Coefficient = 0.5331, non significatif (p = 0.7113), pas d'impact sur la consommation.

    ```         
    Bien que le mod√®le explique une grande partie de la variance de la consommation (R¬≤ √©lev√©), toutes les variables explicatives ne sont pas significatives. Cela peut √™tre d√ª √† une **multicolin√©arit√©** ou √† la taille r√©duite de √©chantillon (14 observations).
    ```

Nous allons passer √† la d√©tection de la multicolin√©arit√© via le VIF et CN.

## b. D√©tection de la multi colin√©arit√©.

```{r}
# V√©rification de la multicolin√©arit√©
## 1. Calcul des VIF
vif_values <- car::vif(model_mco)  # Facteurs d'Inflation de la Variance
# Cr√©ation d'un tableau structur√© avec gt
vif_table <- data.frame(
  Variable = names(vif_values),
  VIF = round(vif_values, 4)
)

# Cr√©ation du tableau gt
vif_gt <- vif_table %>%
  gt() %>%
  tab_header(
    title = md("**Facteurs d'Inflation de la Variance (VIF)**"),
    subtitle = "D√©tection de la multicolin√©arit√©"
  ) %>%
  fmt_number(
    columns = vars(VIF),
    decimals = 4
  ) %>%
  cols_label(
    Variable = "Variable",
    VIF = "Valeur du VIF"
  ) %>%
  tab_style(
    style = cell_borders(sides = "all", color = "gray70", weight = px(1)),
    locations = cells_body()
  ) %>%
  tab_options(
    table.border.top.color = "black",
    table.border.bottom.color = "black"
  )

# Affichage du tableau
vif_gt

```

```{r}
# Extraction de la matrice des variables explicatives (sans l'intercept)
X <- model.matrix(model_mco)[, -1]  # Suppression de la colonne de l'intercept

# Calcul de la matrice X'X
XtX <- t(X) %*% X

# Calcul des valeurs propres de la matrice X'X
eigenvalues <- eigen(XtX)$values  # Extraction des valeurs propres

# Identification des valeurs propres maximales et minimales
eigen_max <- max(eigenvalues)  # Plus grande valeur propre
eigen_min <- min(eigenvalues)  # Plus petite valeur propre

# Calcul du Condition Number (CN) avec racine carr√©e
cn <- sqrt(eigen_max / eigen_min)

# Cr√©ation du tableau des r√©sultats avec arrondi √† 2 d√©cimales
cn_table <- tibble(
  M√©triques = c("Valeur propre maximale", "Valeur propre minimale", "Condition Number (CN)"),
  Valeur = round(c(eigen_max, eigen_min, cn), 2)  # Arrondi √† 2 d√©cimales
)

# Cr√©ation du tableau gt
cn_gt <- cn_table %>%
  gt() %>%
  tab_header(
    title = md("**Condition Number (CN) et Valeurs Propres**"),
    subtitle = "D√©tection de la multicolin√©arit√©"
  ) %>%
  fmt_number(
    columns = vars(Valeur),
    decimals = 2  # Affichage avec 2 d√©cimales
  ) %>%
  cols_label(
    M√©triques = "M√©triques",
    Valeur = "Valeur"
  ) %>%
  tab_style(
    style = cell_borders(sides = "all", color = "gray70", weight = px(1)),
    locations = cells_body()
  ) %>%
  tab_options(
    table.border.top.color = "black",
    table.border.bottom.color = "black"
  )

# Affichage du tableau
cn_gt
# Condition Indices (CI) pour chaque valeur propre (racine de Œª_max / Œª_i)
condition_indices <- sqrt(max(eigenvalues) / eigenvalues)

# Cr√©ation du tableau structur√©
cn_table <- tibble(
  Variable = colnames(X),
  `Condition Index (CI)` = round(condition_indices, 2)
)

# Affichage du tableau avec gt
cn_gt <- cn_table %>%
  gt() %>%
  tab_header(
    title = md("**Condition Indices (CI)**"),
    subtitle = "Mesure de la colin√©arit√© par variable"
  ) %>%
  fmt_number(columns = vars(`Condition Index (CI)`), decimals = 2) %>%
  cols_label(Variable = "Variable", `Condition Index (CI)` = "Condition Index") %>%
  tab_style(
    style = cell_borders(sides = "all", color = "gray70", weight = px(1)),
    locations = cells_body()
  ) %>%
  tab_options(
    table.border.top.color = "black",
    table.border.bottom.color = "black"
  )

# Affichage du tableau
cn_gt

```

**Interpr√©tation:**

Au moins une variable ( wage.income) d√©passe 10, la multicolin√©arit√© est probl√©matique. On a non.farm.income qui proche de 10 √©galement.

## c. Estimation par la r√©gression Ridge ordinaire.

Pour cette estimation, nous allons utiliser le package glemmet qui va nous permettre de calculer et de visualiser les estimateurs.

Il n‚Äôest pas possible d‚Äôutiliser de formule dans la fonction glmnet pour sp√©ciÔ¨Åer la variable √† expliquer et les variables explicatives. Il faut renseigner les variables explicatives dans une matrice. On utilise souvent la fonction model.matrix pour obtenir cette matrice. On aura les estimateurs ridge pour Œ± = 0.

```{r}

# Cr√©er la matrice des pr√©dicteurs
KG.X <- model.matrix(consumption ~ wage.income + non.farm.income + farm.income, data = KG)[,-1]
# D√©finir la variable d'int√©r√™t 
KG.Y <- KG$consumption
# Ajuster la r√©gression Ridge
ridge_model <- glmnet(KG.X, KG.Y, alpha = 0)  # alpha = 0 pour Ridge

# √âtape 4 : Afficher les r√©sultats
ridge_model



```

```{r}
plot(ridge_model,main="Ridge KG en fonction de norme",ylim=c(-2,2))
plot(ridge_model,xvar="lambda",main="Ridge KG en fonction de lambda",ylim=c(-2,2))
```

### choix du param√®tre de r√©gularisation Œª

Nous allons utiliser **cv.glmnet** pour effectuer une validation crois√©e et s√©lectionner la meilleure valeur de p√©nalisation (Œª)

```{r}
# Validation crois√©e pour trouver les meilleurs lambda
cv_ridge <- cv.glmnet(KG.X, KG.Y, alpha = 0)


# Visualisation des erreurs en fonction de log(Œª)
plot(cv_ridge, main = "Validation crois√©e Ridge KG")



lambda_min <- cv_ridge$lambda.min   # Lambda minimisant l'erreur
lambda_1se <- cv_ridge$lambda.1se   # Lambda plus simple dans la borne 1SE

cat("Meilleur lambda :", round(lambda_min, 2), "et", round(lambda_1se, 2), "\n")

# √âtape 4 : Ajuster la r√©gression Ridge avec lmridge pour les deux lambdas
ridge_model <- lmridge(consumption ~ wage.income + non.farm.income + farm.income, 
                        data = KG, K = c(0,2.47, 14.45), scaling = "scaled")

# √âtape 5 : Afficher le r√©sum√© des mod√®les Ridge
summary(ridge_model)
vif.lmridge(ridge_model)
rstats1.lmridge(ridge_model)$CN
```

## 3. Comparaison des Moindres Carr√©s Ordinaires et de la r√©gression Ridge sur des donn√©es

L'objectif de cette section est de comparer les performances des mod√®les MCO et Ridge en analysant la **pr√©cision des coefficients estim√©s**, leur **significativit√© statistique**, et l‚Äôimpact de la **r√©gularisation Ridge sur la multicolin√©arit√©**. En nous basant sur la litt√©rature existante, nous √©valuerons si l'utilisation de Ridge permet d'am√©liorer l‚Äôinterpr√©tation des r√©sultats et de stabiliser les estimations.

```{r}
# Valeurs de lambda sp√©cifi√©es
lambda_values <- c(2.47, 14.45)

# Ajuster le mod√®le Ridge pour les deux valeurs de lambda
ridge_model_t <- lmridge(consumption ~ wage.income + non.farm.income + farm.income, 
                        data = KG, K = lambda_values, scaling = "scaled")

#  1. Calcul des VIFs pour chaque lambda
vif_values <- vif.lmridge(ridge_model_t)

# Cr√©er un tableau des VIFs
vif_table <- data.frame(
  Variable = colnames(KG[, -1]),  # Exclut la variable d√©pendante
  VIF_Lambda_2.47 = vif_values[1, ],  # VIF pour lambda = 2.47
  VIF_Lambda_14.45 = vif_values[2, ]  # VIF pour lambda = 14.45
)

# Afficher le tableau des VIFs avec gt
vif_gt <- vif_table %>%
  gt() %>%
  tab_header(
    title = md("**Facteurs d'Inflation de la Variance (VIF) apr√®s Ridge**"),
    subtitle = "D√©tection de la multicolin√©arit√© apr√®s r√©gularisation"
  ) %>%
  fmt_number(columns = c(VIF_Lambda_2.47, VIF_Lambda_14.45), decimals = 2)

vif_gt

# üìå 2. Calcul du Condition Number (CN) apr√®s Ridge
cn_values <- rstats1.lmridge(ridge_model_t)$CN

# Cr√©er un tableau pour CN
cn_table <- data.frame(
  M√©trique = c("Condition Number (CN)"),
  CN_Lambda_2.47 = cn_values[1],
  CN_Lambda_14.45 = cn_values[2]
)

# Afficher le tableau CN avec gt
cn_gt <- cn_table %>%
  gt() %>%
  tab_header(
    title = md("**Condition Number (CN) apr√®s Ridge**"),
    subtitle = "D√©tection de la multicolin√©arit√© apr√®s r√©gularisation"
  ) %>%
  fmt_number(columns = c(CN_Lambda_2.47, CN_Lambda_14.45), decimals = 2)

cn_gt


```

Apr√®s l‚Äôapplication de la r√©gression Ridge, les effets de la multicolin√©arit√© disparaissent presque totalement. Avec Œª = 2.47, les VIF chutent √† des valeurs tr√®s faibles, autour de 0.04 √† 0.06, tandis que le CN diminue drastiquement √† 2.04. Cette r√©duction du CN indique que la matrice des pr√©dicteurs est maintenant bien conditionn√©e, rendant les estimations des coefficients beaucoup plus stables. En augmentant davantage Œª √† 14.45, l‚Äôeffet de r√©gularisation devient encore plus fort, avec des VIF qui tombent √† z√©ro et un CN de seulement 1.18. Cela signifie que toutes les corr√©lations probl√©matiques entre les variables explicatives ont √©t√© corrig√©es, garantissant un mod√®le bien plus robuste.

Ainsi, la comparaison entre le MCO et la r√©gression Ridge montre clairement l‚Äôefficacit√© de la r√©gularisation pour stabiliser les estimations. Alors que le MCO souffre d‚Äôun probl√®me important de colin√©arit√©, Ridge permet de rendre le mod√®le plus fiable. Toutefois, un Œª trop √©lev√© peut lisser excessivement les coefficients, ce qui peut poser probl√®me si l‚Äôon cherche √† interpr√©ter leurs impacts individuels. Dans ce contexte, Œª = 2.47 semble √™tre un bon compromis, car il r√©duit la colin√©arit√© sans trop alt√©rer la structure du mod√®le.

```{r}
# Extraction des r√©sultats MCO (OLS)
summary_mco <- summary(model_mco)
coeff_mco <- summary_mco$coefficients
mse_mco <- mean(summary_mco$residuals^2)
r2_mco <- summary_mco$r.squared

# Extraction des statistiques Ridge avec rstats1.lmridge()
stats_ridge <- rstats1.lmridge(ridge_model_t)

# Extraction des r√©sultats Ridge pour Œª = 2.47
summary_ridge_2.47 <- summary(ridge_model_t)$summaries[[1]]
coeff_ridge_2.47 <- summary_ridge_2.47$coefficients[, "Estimate"]
se_ridge_2.47 <- summary_ridge_2.47$coefficients[, "StdErr (Sc)"]
t_ridge_2.47 <- summary_ridge_2.47$coefficients[, "t-value (Sc)"]
p_ridge_2.47 <- summary_ridge_2.47$coefficients[, "Pr(>|t|)"]

mse_ridge_2.47 <- stats_ridge$mse[1]
r2_ridge_2.47 <- stats_ridge$R2[1]

# Extraction des r√©sultats Ridge pour Œª = 14.45
summary_ridge_14.45 <- summary(ridge_model_t)$summaries[[2]]
coeff_ridge_14.45 <- summary_ridge_14.45$coefficients[, "Estimate"]
se_ridge_14.45 <- summary_ridge_14.45$coefficients[, "StdErr (Sc)"]
t_ridge_14.45 <- summary_ridge_14.45$coefficients[, "t-value (Sc)"]
p_ridge_14.45 <- summary_ridge_14.45$coefficients[, "Pr(>|t|)"]

mse_ridge_14.45 <- stats_ridge$mse[2]
r2_ridge_14.45 <- stats_ridge$R2[2]

# Cr√©ation des tableaux et affichage
table_mco <- data.frame(
  Variable = rownames(coeff_mco),
  Coefficient = coeff_mco[,1],
  Standard_Error = coeff_mco[,2],
  T_value = coeff_mco[,3],
  P_value = coeff_mco[,4]
)

table_mco <- rbind(table_mco, 
                   c("MSE", mse_mco, "", "", ""),
                   c("R¬≤", r2_mco, "", "", ""))

table_ridge_2.47 <- data.frame(
  Variable = rownames(summary_ridge_2.47$coefficients),
  Coefficient = coeff_ridge_2.47,
  Standard_Error = se_ridge_2.47,
  T_value = t_ridge_2.47,
  P_value = p_ridge_2.47
)

table_ridge_2.47 <- rbind(table_ridge_2.47, 
                          c("MSE", mse_ridge_2.47, "", "", ""),
                          c("R¬≤", r2_ridge_2.47, "", "", ""))

table_ridge_14.45 <- data.frame(
  Variable = rownames(summary_ridge_14.45$coefficients),
  Coefficient = coeff_ridge_14.45,
  Standard_Error = se_ridge_14.45,
  T_value = t_ridge_14.45,
  P_value = p_ridge_14.45
)

table_ridge_14.45 <- rbind(table_ridge_14.45, 
                           c("MSE", mse_ridge_14.45, "", "", ""),
                           c("R¬≤", r2_ridge_14.45, "", "", ""))

# Formatage des tableaux avec gt()
gt_mco <- table_mco %>%
  gt() %>%
  tab_header(
    title = md("**R√©sultats de la r√©gression MCO (OLS)**"),
    subtitle = "Coefficients, erreurs standard, t-values et p-values"
  ) %>%
  fmt_number(columns = c(Coefficient, Standard_Error, T_value, P_value), decimals = 4)

gt_ridge_2.47 <- table_ridge_2.47 %>%
  gt() %>%
  tab_header(
    title = md("**R√©sultats de la r√©gression Ridge (Œª = 2.47)**"),
    subtitle = "Coefficients, erreurs standard, t-values et p-values"
  ) %>%
  fmt_number(columns = c(Coefficient, Standard_Error, T_value, P_value), decimals = 4)

gt_ridge_14.45 <- table_ridge_14.45 %>%
  gt() %>%
  tab_header(
    title = md("**R√©sultats de la r√©gression Ridge (Œª = 14.45)**"),
    subtitle = "Coefficients, erreurs standard, t-values et p-values"
  ) %>%
  fmt_number(columns = c(Coefficient, Standard_Error, T_value, P_value), decimals = 4)

# Affichage des tableaux
gt_mco
gt_ridge_2.47
gt_ridge_14.45

```

Dans le mod√®le MCO, les coefficients sont relativement √©lev√©s, mais plusieurs ne sont pas significatifs. Par exemple, *wage.income* et *farm.income* pr√©sentent des p-values √©lev√©es (respectivement 0.25 et 0.71), ce qui signifie qu'ils ne sont pas significatifs au seuil de 5 %. Le R¬≤ de 0.918 montre que le mod√®le explique une grande part de la variance des donn√©es, mais cette performance est potentiellement biais√©e par la multicolin√©arit√©. Le MSE, relativement bas √† 26.23, sugg√®re un bon ajustement, mais l'instabilit√© des coefficients due √† la colin√©arit√© peut affecter leur interpr√©tabilit√©.

Avec Ridge et Œª = 2.47, on constate une r√©duction des coefficients, ce qui est attendu avec la r√©gularisation. Toutefois, contrairement au MCO, les variables explicatives deviennent plus significatives, avec des p-values tr√®s faibles. Cela indique que la p√©nalisation aide √† r√©duire l‚Äôimpact de la colin√©arit√© et stabilise les coefficients. En contrepartie, le MSE augmente fortement √† 763.34, et le R¬≤ chute √† 0.239, indiquant une baisse de la capacit√© pr√©dictive du mod√®le. Cela illustre le compromis de Ridge : une meilleure stabilit√© des coefficients contre une moindre capacit√© d'ajustement aux donn√©es.

Lorsque Œª est pouss√© √† 14.45, l'effet de r√©gularisation est encore plus marqu√©. Les coefficients sont encore plus r√©duits, et les erreurs standard diminuent, traduisant une plus grande robustesse du mod√®le. Cependant, le MSE explose √† 1552.17 et le R¬≤ tombe √† seulement 0.021, signifiant que le mod√®le capture √† peine la variance des donn√©es. Cela montre qu‚Äôune r√©gularisation excessive √©limine non seulement la multicolin√©arit√© mais r√©duit √©galement drastiquement la capacit√© pr√©dictive.

En conclusion, le MCO est performant en termes de R¬≤ et de MSE mais souffre fortement de la multicolin√©arit√©, ce qui rend ses coefficients peu fiables. Ridge Œª = 2.47 est un bon compromis, en am√©liorant la significativit√© des coefficients et en r√©duisant la colin√©arit√©, bien que la perte de R¬≤ soit notable. Ridge Œª = 14.45 pousse la r√©gularisation √† l'extr√™me, r√©duisant davantage la variance des coefficients mais au prix d'une tr√®s faible capacit√© pr√©dictive. Ainsi, si l'objectif est d'avoir un mod√®le robuste et interpr√©table tout en minimisant les effets de la colin√©arit√©, Ridge avec Œª = 2.47 semble √™tre le meilleur choix.

```{r}
# Charger le dataset
data(longley)

# Afficher un aper√ßu des donn√©es
head(longley)
# R√©gression lin√©aire multiple (MCO)
modele <- lm(Employed ~ GNP + GNP.deflator + Unemployed + Armed.Forces + Population + Year, data = longley)

# Afficher un r√©sum√© des r√©sultats
summary(modele)
vif(modele)
```
